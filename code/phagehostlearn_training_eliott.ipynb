{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e97849c",
   "metadata": {},
   "source": [
    "## 1. Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "general_path = os.path.join(project_root, 'data')\n",
    "results_path = os.path.join(project_root, 'results')\n",
    "data_suffix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities originally defined in phagehostlearn_processing.py\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from os import listdir\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SearchIO import HmmerIO\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60092310",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "\n",
    "The data processing of PhageHostLearn consists of five consecutive steps: (1) phage gene calling with PHANOTATE, (2) phage protein embedding with bio_embeddings, (3) phage RBP detection, (4) bacterial genome processing with Kaptive and (5) processing the interaction matrix.\n",
    "\n",
    "Expected outputs: (1) an RBPbase.csv file with detected RBPs, (2) a Locibase.json file with detected K-loci proteins, (3) a .csv file of processed interaction data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc090f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmmpress_python(hmm_path, pfam_file):\n",
    "    \"\"\"Press a profiles database, necessary to do scanning.\"\"\"\n",
    "    cd_str = 'cd ' + hmm_path\n",
    "    press_str = 'hmmpress ' + pfam_file\n",
    "    command = cd_str + '; ' + press_str\n",
    "    press_process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    press_out, press_err = press_process.communicate()\n",
    "    return press_out, press_err\n",
    "\n",
    "def single_hmmscan_python(hmm_path, pfam_file, fasta_file):\n",
    "    \"\"\"Run hmmscan for a given FASTA file of one (or multiple) sequences.\"\"\"\n",
    "    cd_str = 'cd ' + hmm_path\n",
    "    cd_process = subprocess.Popen(cd_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    cd_process.communicate()\n",
    "\n",
    "    scan_str = 'hmmscan ' + pfam_file + ' ' + fasta_file + ' > hmmscan_out.txt'\n",
    "    scan_process = subprocess.Popen(scan_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    scan_process.communicate()\n",
    "\n",
    "    with open('hmmscan_out.txt') as results_handle:\n",
    "        scan_res = HmmerIO.Hmmer3TextParser(results_handle)\n",
    "    os.remove('hmmscan_out.txt')\n",
    "    return scan_res\n",
    "\n",
    "\n",
    "def hmmscan_python(hmm_path, pfam_file, sequences_file, threshold=18):\n",
    "    \"\"\"Scan sequences for domains using hmmscan.\"\"\"\n",
    "    domains = []\n",
    "    scores = []\n",
    "    biases = []\n",
    "    ranges = []\n",
    "    for sequence in SeqIO.parse(sequences_file, 'fasta'):\n",
    "        with open('single_sequence.fasta', 'w') as temp_fasta:\n",
    "            temp_fasta.write('>' + sequence.id + '\\n' + str(sequence.seq) + '\\n')\n",
    "\n",
    "        scan_res = single_hmmscan_python(hmm_path, pfam_file, 'single_sequence.fasta')\n",
    "        for line in scan_res:\n",
    "            try:\n",
    "                for hit in line.hits:\n",
    "                    hsp = hit._items[0]\n",
    "                    aln_start = hsp.query_range[0]\n",
    "                    aln_stop = hsp.query_range[1]\n",
    "                    if (hit.bitscore >= threshold) and (hit.id not in domains):\n",
    "                        domains.append(hit.id)\n",
    "                        scores.append(hit.bitscore)\n",
    "                        biases.append(hit.bias)\n",
    "                        ranges.append((aln_start, aln_stop))\n",
    "            except IndexError:\n",
    "                pass\n",
    "    os.remove('single_sequence.fasta')\n",
    "    return domains, scores, biases, ranges\n",
    "\n",
    "\n",
    "def gene_domain_scan(hmmpath, pfam_file, gene_hits, threshold=18):\n",
    "    \"\"\"Run hmmscan on translated gene hits.\"\"\"\n",
    "    with open('protein_hits.fasta', 'w') as hits_fasta:\n",
    "        for i, gene_hit in enumerate(gene_hits):\n",
    "            protein_sequence = str(Seq(gene_hit).translate())[:-1]\n",
    "            hits_fasta.write('>' + str(i) + '_proteindomain_hit\\n' + protein_sequence + '\\n')\n",
    "    domains, scores, biases, ranges = hmmscan_python(hmmpath, pfam_file, 'protein_hits.fasta', threshold)\n",
    "    os.remove('protein_hits.fasta')\n",
    "    return domains, scores, biases, ranges\n",
    "\n",
    "\n",
    "def kaptive_python(database_path, file_path, output_path):\n",
    "    \"\"\"Wrapper for the Kaptive command-line call.\"\"\"\n",
    "    command = 'python kaptive.py -a ' + file_path + ' -k ' + database_path + ' -o ' + output_path + '/ --no_table'\n",
    "    ssprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    ssprocess.communicate()\n",
    "    return\n",
    "\n",
    "\n",
    "def xlsx_database_to_csv(xlsx_file, save_path, index_col=0, header=0, export=True):\n",
    "    \"\"\"Convert an XLSX interaction matrix to CSV.\"\"\"\n",
    "    interactions_matrix = pd.read_excel(xlsx_file, index_col=index_col, header=header)\n",
    "    if export:\n",
    "        interactions_matrix.to_csv(save_path + '.csv')\n",
    "        return\n",
    "    return interactions_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b686db",
   "metadata": {},
   "source": [
    "#### 2.1 PHANOTATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phanotate_processing(general_path, phage_genomes_path, phanotate_path, data_suffix='', add=False, test=False, num_phages=None):\n",
    "    \"\"\"Run PHANOTATE on each phage genome and build the gene database.\"\"\"\n",
    "    phage_files = listdir(phage_genomes_path)\n",
    "    print('Number of phage files:', len(phage_files))\n",
    "    if '.DS_Store' in phage_files:\n",
    "        phage_files.remove('.DS_Store')\n",
    "    if add:\n",
    "        rbp_base = pd.read_csv(general_path + '/RBPbase' + data_suffix + '.csv')\n",
    "        phage_ids = list(set(rbp_base['phage_ID']))\n",
    "        phage_files = [x for x in phage_files if x.split('.fasta')[0] not in phage_ids]\n",
    "        print('Processing ', len(phage_files), ' more phages (add=True)')\n",
    "    if num_phages is not None:\n",
    "        print('Processing only the first ', num_phages, ' phages')\n",
    "        phage_files = phage_files[:num_phages]\n",
    "    bar = tqdm(total=len(phage_files), position=0, leave=True, desc='Processing phage genomes')\n",
    "    name_list = []\n",
    "    gene_list = []\n",
    "    gene_ids = []\n",
    "\n",
    "    for file in phage_files:\n",
    "        count = 1\n",
    "        file_dir = phage_genomes_path + '/' + file\n",
    "        raw_str = phanotate_path + ' ' + file_dir\n",
    "        process = subprocess.Popen(raw_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        stdout, _ = process.communicate()\n",
    "        stdout_text = stdout.decode('utf-8', errors='ignore')\n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"PHANOTATE command failed for {file_dir} with exit code {process.returncode}. \"\n",
    "                f\"Command output:\\n{stdout_text}\"\n",
    "            )\n",
    "        std_splits = stdout.split(sep=b'\\n')[2:]\n",
    "        if not any(split.strip() for split in std_splits):\n",
    "            raise ValueError(\n",
    "                f\"PHANOTATE did not return ORF predictions for {file_dir}. \"\n",
    "                f\"Command output:\\n{stdout_text}\"\n",
    "            )\n",
    "\n",
    "        temp_tab_path = os.path.join(general_path, 'phage_results.tsv')\n",
    "        with open(temp_tab_path, 'wb') as temp_tab:\n",
    "            for split in std_splits:\n",
    "                split = split.replace(b',', b'')\n",
    "                temp_tab.write(split + b'\\n')\n",
    "        try:\n",
    "            results_orfs = pd.read_csv(temp_tab_path, sep='\\t', lineterminator='\\n', index_col=False)\n",
    "        except pd.errors.EmptyDataError as exc:\n",
    "            with open(temp_tab_path, 'r', encoding='utf-8', errors='ignore') as temp_in:\n",
    "                temp_preview = temp_in.read()\n",
    "            raise ValueError(\n",
    "                f\"PHANOTATE output for {file_dir} produced an empty or invalid TSV file.\\n\"\n",
    "                f\"Command output:\\n{stdout_text}\\n\"\n",
    "                f\"Temporary TSV content:\\n{temp_preview}\"\n",
    "            ) from exc\n",
    "\n",
    "        name = file.split('.fasta')[0]\n",
    "        sequence = str(SeqIO.read(file_dir, 'fasta').seq)\n",
    "        for j, strand in enumerate(results_orfs['FRAME']):\n",
    "            start = results_orfs['#START'][j]\n",
    "            stop = results_orfs['STOP'][j]\n",
    "            if strand == '+':\n",
    "                gene = sequence[start - 1:stop]\n",
    "            else:\n",
    "                sequence_part = sequence[stop - 1:start]\n",
    "                gene = str(Seq(sequence_part).reverse_complement())\n",
    "            name_list.append(name)\n",
    "            gene_list.append(gene)\n",
    "            gene_ids.append(name + '_gp' + str(count))\n",
    "            count += 1\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    if not test and os.path.exists(os.path.join(general_path, 'phage_results.tsv')):\n",
    "        os.remove(os.path.join(general_path, 'phage_results.tsv'))\n",
    "\n",
    "    genebase = pd.DataFrame(list(zip(name_list, gene_ids, gene_list)), columns=['phage_ID', 'gene_ID', 'gene_sequence'])\n",
    "    if add:\n",
    "        old_genebase = pd.read_csv(general_path + '/phage_genes' + data_suffix + '.csv')\n",
    "        genebase = pd.concat([old_genebase, genebase], axis=0)\n",
    "    genebase.to_csv(general_path + '/phage_genes' + data_suffix + '.csv', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d633972",
   "metadata": {},
   "outputs": [],
   "source": [
    "phage_genomes_path = general_path+'/phages_genomes'\n",
    "phanotate_path = '/Users/eliottvalette/Documents/Clones/PhageHostLearn/.venv/bin/phanotate.py'\n",
    "phanotate_processing(general_path, phage_genomes_path, phanotate_path, data_suffix=data_suffix, num_phages=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a48369",
   "metadata": {},
   "source": [
    "#### 2.2 Protein embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd322693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_protein_embeddings(general_path, data_suffix='', add=False, num_genes=None):\n",
    "    \"\"\"Compute protein embeddings using ProtTransBertBFD.\"\"\"\n",
    "    genebase = pd.read_csv(general_path + '/phage_genes' + data_suffix + '.csv')\n",
    "    if num_genes is not None:\n",
    "        print('Processing only the first ', num_genes, ' phage genes')\n",
    "        genebase = genebase.head(num_genes)\n",
    "    print('Number of phage genes:', len(genebase))\n",
    "    time_start = time.time()\n",
    "    embedder = ProtTransBertBFDEmbedder()\n",
    "    time_end = time.time()\n",
    "    print('Time taken to initialize embedder:', time_end - time_start)\n",
    "    print('Embedder initialized')\n",
    "    if add:\n",
    "        print('Adding new protein embeddings')\n",
    "        old_embeddings_df = pd.read_csv(general_path + '/phage_protein_embeddings' + data_suffix + '.csv')\n",
    "        protein_ids = list(old_embeddings_df['ID'])\n",
    "        sequences = []\n",
    "        names = []\n",
    "        for i, sequence in enumerate(genebase['gene_sequence']):\n",
    "            if genebase['gene_ID'][i] not in protein_ids:\n",
    "                sequences.append(str(Seq(sequence).translate())[:-1])\n",
    "                names.append(genebase['gene_ID'][i])\n",
    "    else:\n",
    "        print('Computing protein embeddings for all phage genes')\n",
    "        names = list(genebase['gene_ID'])\n",
    "        print('Number of protein sequences to embed:', len(names))\n",
    "        sequences = [str(Seq(sequence).translate())[:-1] for sequence in genebase['gene_sequence']]\n",
    "\n",
    "    print('Number of protein sequences to embed:', len(sequences))\n",
    "    protein_embeddings = []\n",
    "    progress_bar = tqdm(sequences, desc='Computing protein embeddings', unit='protein')\n",
    "    for protein_sequence in progress_bar:\n",
    "        reduced_embedding = embedder.reduce_per_protein(embedder.embed(protein_sequence))\n",
    "        protein_embeddings.append(reduced_embedding)\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'ID': names}), pd.DataFrame(protein_embeddings)], axis=1)\n",
    "    if add:\n",
    "        embeddings_df = pd.DataFrame(np.vstack([old_embeddings_df, embeddings_df]), columns=old_embeddings_df.columns)\n",
    "    embeddings_df.to_csv(general_path + '/phage_protein_embeddings' + data_suffix + '.csv', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62489ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_protein_embeddings(general_path, data_suffix=data_suffix, num_genes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07fe07",
   "metadata": {},
   "source": [
    "#### 2.3 PhageRBPdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a181bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def phageRBPdetect(general_path, pfam_path, hmmer_path, xgb_path, gene_embeddings_path, data_suffix=''):\n",
    "    \"\"\"Detect receptor-binding proteins using PhageRBPdetect.\"\"\"\n",
    "    genebase = pd.read_csv(general_path + '/phage_genes' + data_suffix + '.csv')\n",
    "    new_blocks = ['Phage_T7_tail', 'Tail_spike_N', 'Prophage_tail', 'BppU_N', 'Mtd_N', 'Head_binding', 'DUF3751',\n",
    "                  'End_N_terminal', 'phage_tail_N', 'Prophage_tailD1', 'DUF2163', 'Phage_fiber_2', 'unknown_N0',\n",
    "                  'unknown_N1', 'unknown_N2', 'unknown_N3', 'unknown_N4', 'unknown_N6', 'unknown_N10', 'unknown_N11',\n",
    "                  'unknown_N12', 'unknown_N13', 'unknown_N17', 'unknown_N19', 'unknown_N23', 'unknown_N24',\n",
    "                  'unknown_N26', 'unknown_N29', 'unknown_N36', 'unknown_N45', 'unknown_N48', 'unknown_N49',\n",
    "                  'unknown_N53', 'unknown_N57', 'unknown_N60', 'unknown_N61', 'unknown_N65', 'unknown_N73',\n",
    "                  'unknown_N82', 'unknown_N83', 'unknown_N101', 'unknown_N114', 'unknown_N119', 'unknown_N122',\n",
    "                  'unknown_N163', 'unknown_N174', 'unknown_N192', 'unknown_N200', 'unknown_N206', 'unknown_N208',\n",
    "                  'Lipase_GDSL_2', 'Pectate_lyase_3', 'gp37_C', 'Beta_helix', 'Gp58', 'End_beta_propel',\n",
    "                  'End_tail_spike', 'End_beta_barrel', 'PhageP22-tail', 'Phage_spike_2', 'gp12-short_mid', 'Collar',\n",
    "                  'unknown_C2', 'unknown_C3', 'unknown_C8', 'unknown_C15', 'unknown_C35', 'unknown_C54', 'unknown_C76',\n",
    "                  'unknown_C100', 'unknown_C105', 'unknown_C112', 'unknown_C123', 'unknown_C179', 'unknown_C201',\n",
    "                  'unknown_C203', 'unknown_C228', 'unknown_C234', 'unknown_C242', 'unknown_C258', 'unknown_C262',\n",
    "                  'unknown_C267', 'unknown_C268', 'unknown_C274', 'unknown_C286', 'unknown_C292', 'unknown_C294',\n",
    "                  'Peptidase_S74', 'Phage_fiber_C', 'S_tail_recep_bd', 'CBM_4_9', 'DUF1983', 'DUF3672']\n",
    "\n",
    "    output, err = hmmpress_python(hmmer_path, pfam_path)\n",
    "    print(output)\n",
    "\n",
    "    phage_genes = genebase['gene_sequence']\n",
    "    hmm_scores = {item: [0] * len(phage_genes) for item in new_blocks}\n",
    "    bar = tqdm(total=len(phage_genes), position=0, leave=True, desc='Scanning phage genes')\n",
    "    for i, sequence in enumerate(phage_genes):\n",
    "        hits, scores, biases, ranges = gene_domain_scan(hmmer_path, pfam_path, [sequence])\n",
    "        for j, dom in enumerate(hits):\n",
    "            hmm_scores[dom][i] = scores[j]\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "    hmm_scores_array = np.asarray(pd.DataFrame(hmm_scores))\n",
    "\n",
    "    embeddings_df = pd.read_csv(gene_embeddings_path)\n",
    "    embeddings = np.asarray(embeddings_df.iloc[:, 1:])\n",
    "    features = np.concatenate((embeddings, hmm_scores_array), axis=1)\n",
    "\n",
    "    xgb_saved = XGBClassifier()\n",
    "    xgb_saved.load_model(xgb_path)\n",
    "\n",
    "    score_xgb = xgb_saved.predict_proba(features)[:, 1]\n",
    "    preds_xgb = (score_xgb > 0.5) * 1\n",
    "\n",
    "    rbp_base = {'phage_ID': [], 'protein_ID': [], 'protein_sequence': [], 'dna_sequence': [], 'xgb_score': []}\n",
    "    for i, dna_sequence in enumerate(genebase['gene_sequence']):\n",
    "        if preds_xgb[i] == 1:\n",
    "            rbp_base['phage_ID'].append(genebase['phage_ID'][i])\n",
    "            rbp_base['protein_ID'].append(genebase['gene_ID'][i])\n",
    "            rbp_base['protein_sequence'].append(str(Seq(dna_sequence).translate())[:-1])\n",
    "            rbp_base['dna_sequence'].append(dna_sequence)\n",
    "            rbp_base['xgb_score'].append(score_xgb[i])\n",
    "    rbp_base_df = pd.DataFrame(rbp_base)\n",
    "    to_delete = [i for i, protein_seq in enumerate(rbp_base_df['protein_sequence']) if (len(protein_seq) < 200 or len(protein_seq) > 1500)]\n",
    "    rbp_base_df = rbp_base_df.drop(to_delete).reset_index(drop=True)\n",
    "    rbp_base_df.to_csv(general_path + '/RBPbase' + data_suffix + '.csv', index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfam_path = general_path+'/RBPdetect_phageRBPs.hmm'\n",
    "hmmer_path = '/Users/Dimi/hmmer-3.3.1'\n",
    "xgb_path = general_path+'/RBPdetect_xgb_hmm.json'\n",
    "gene_embeddings_path = general_path+'/phage_protein_embeddings'+data_suffix+'.csv'\n",
    "phageRBPdetect(general_path, pfam_path, hmmer_path, xgb_path, gene_embeddings_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed4b0c",
   "metadata": {},
   "source": [
    "#### 2.4 Kaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ae7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bacterial_genomes(general_path, bact_genomes_path, database_path, data_suffix='', add=False):\n",
    "    \"\"\"Process bacterial genomes with Kaptive to extract K-locus proteins.\"\"\"\n",
    "    fastas = listdir(bact_genomes_path)\n",
    "    try:\n",
    "        fastas.remove('.DS_Store')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if add:\n",
    "        with open(general_path + '/Locibase' + data_suffix + '.json') as dict_file:\n",
    "            old_locibase = json.load(dict_file)\n",
    "        loci_accessions = list(old_locibase.keys())\n",
    "        fastas = [x for x in fastas if x.split('.fasta')[0] not in loci_accessions]\n",
    "        print('Processing ', len(fastas), ' more bacteria (add=True)')\n",
    "    accessions = [file.split('.fasta')[0] for file in fastas]\n",
    "    serotypes = []\n",
    "    loci_results = {}\n",
    "    pbar = tqdm(total=len(fastas), desc='Processing bacterial genomes')\n",
    "    with open(general_path + '/kaptive_results_all_loci.fasta', 'w') as big_fasta:\n",
    "        for i, file in enumerate(fastas):\n",
    "            file_path = bact_genomes_path + '/' + file\n",
    "            kaptive_python(database_path, file_path, general_path)\n",
    "\n",
    "            results = json.load(open(general_path + '/kaptive_results.json'))\n",
    "            serotypes.append(results[0]['Best match']['Type'])\n",
    "            for gene in results[0]['Locus genes']:\n",
    "                try:\n",
    "                    protein = gene['tblastn result']['Protein sequence']\n",
    "                    protein = protein.replace('-', '').replace('*', '')\n",
    "                except KeyError:\n",
    "                    protein = gene['Reference']['Protein sequence']\n",
    "                loci_results.setdefault(accessions[i], []).append(protein[:-1])\n",
    "\n",
    "            loci_sequence = ''\n",
    "            for record in SeqIO.parse(general_path + '/kaptive_results_' + file, 'fasta'):\n",
    "                loci_sequence += str(record.seq)\n",
    "            big_fasta.write('>' + accessions[i] + '\\n' + loci_sequence + '\\n')\n",
    "\n",
    "            for extension in ['.ndb', '.not', '.ntf', '.nto']:\n",
    "                os.remove(file_path + extension)\n",
    "            os.remove(general_path + '/kaptive_results.json')\n",
    "            os.remove(general_path + '/kaptive_results_' + file)\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    sero_df = pd.DataFrame(serotypes, columns=['sero'])\n",
    "    if add:\n",
    "        loci_results = {**old_locibase, **loci_results}\n",
    "        old_seros = pd.read_csv(general_path + '/serotypes' + data_suffix + '.csv')\n",
    "        sero_df = pd.concat([old_seros, sero_df], axis=0)\n",
    "    sero_df.to_csv(general_path + '/serotypes' + data_suffix + '.csv', index=False)\n",
    "    with open(general_path + '/Locibase' + data_suffix + '.json', 'w') as dict_file:\n",
    "        json.dump(loci_results, dict_file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be319ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_genomes_path = general_path+'/klebsiella_genomes/fasta_files'\n",
    "kaptive_database_path = general_path+'/Klebsiella_k_locus_primary_reference.gbk'\n",
    "process_bacterial_genomes(general_path, bact_genomes_path, kaptive_database_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59d1d1",
   "metadata": {},
   "source": [
    "#### 2.5 Process the interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b78216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interactions(general_path, interactions_xlsx_path, data_suffix=''):\n",
    "    \"\"\"Process the interaction matrix and export it to CSV.\"\"\"\n",
    "    output = general_path + '/phage_host_interactions' + data_suffix\n",
    "    xlsx_database_to_csv(interactions_xlsx_path, output)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_xlsx_path = general_path+'/klebsiella_phage_host_interactions.xlsx'\n",
    "process_interactions(general_path, interactions_xlsx_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915feb3",
   "metadata": {},
   "source": [
    "If you want to combine separate data sources of interactions, you can use the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46fdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "phage_genomes_path = general_path+'/phages_genomes'\n",
    "phanotate_path = '/Users/eliottvalette/Documents/Clones/PhageHostLearn/.venv/bin/phanotate.py'\n",
    "phanotate_processing(general_path, phage_genomes_path, phanotate_path, data_suffix=data_suffix, num_phages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f66064",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = general_path+'/phage_host_interactions'+data_suffix\n",
    "new_file = general_path+'/klebsiella_interactions_part2.xlsx' # part 2\n",
    "add_to_database(output+'.csv', new_file, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13771b",
   "metadata": {},
   "source": [
    "## 3. Feature construction\n",
    "\n",
    "Starts from the RBPbase.csv and the Locibase.json files that should be stored in the general_path. If you wish to reproduce our analyses, you can download these files from our [Zenodo repository](https://doi.org/10.5281/zenodo.8095914).\n",
    "\n",
    "Expected outputs: (1) a .csv file with RBP embeddings, (2) a .csv file with loci embeddings. The last function outputs the following Python objects: ESM-2 feature matrix, labels, groups_loci and groups_phage (for evaluation). If the ESM-2 embeddings take too long, you might opt to do this step in the cloud or on a high-performance computer.\n",
    "\n",
    "If you're retraining a model with the same data but new validated interactions, you can simply run the `construct_feature_matrices` function to construct updated feature matrices and labels and train models anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be94f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phagehostlearn_features as phlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34687caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "phlf.compute_esm2_embeddings_rbp(general_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94526a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "phlf.compute_esm2_embeddings_loci(general_path, data_suffix=data_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af80705",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_embeddings_path = general_path+'/esm2_embeddings_rbp'+data_suffix+'.csv'\n",
    "loci_embeddings_path = general_path+'/esm2_embeddings_loci'+data_suffix+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_esm2, labels, groups_loci, groups_phage = phlf.construct_feature_matrices(general_path, \n",
    "                                                                            data_suffix, loci_embeddings_path, \n",
    "                                                                            rbp_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b57a",
   "metadata": {},
   "source": [
    "## 4. Training and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import phagehostlearn_utils as phlu\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, roc_curve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416719a1",
   "metadata": {},
   "source": [
    "#### 4.1 Training both models and saving them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus=6\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 FEATURES + XGBoost model\n",
    "imbalance = sum([1 for i in labels if i==1]) / sum([1 for i in labels if i==0])\n",
    "xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=7,\n",
    "                    n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb.fit(features_esm2, labels)\n",
    "xgb.save_model('phagehostlearn_vbeta.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0143c",
   "metadata": {},
   "source": [
    "#### 4.2 LOGOCV with the combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da198e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to set a threshold for grouping\n",
    "matrix = np.loadtxt(general_path+'/all_loci_score_matrix.txt', delimiter='\\t')\n",
    "threshold = 0.995\n",
    "threshold_str='995'\n",
    "group_i = 0\n",
    "new_groups = [np.nan] * len(groups_loci)\n",
    "for i in range(matrix.shape[0]):\n",
    "    cluster = np.where(matrix[i,:] >= threshold)[0]\n",
    "    oldgroups_i = [k for k, x in enumerate(groups_loci) if x in cluster]\n",
    "    if np.isnan(new_groups[groups_loci.index(i)]):\n",
    "        for ogi in oldgroups_i:\n",
    "            new_groups[ogi] = group_i\n",
    "        group_i += 1\n",
    "groups_loci = new_groups\n",
    "print('Number of unique groups: ', len(set(groups_loci)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "cpus = 6\n",
    "scores_lan = []\n",
    "label_list = []\n",
    "labels = np.asarray(labels)\n",
    "pbar = tqdm(total=len(set(groups_loci)))\n",
    "for train_index, test_index in logo.split(features_esm2, labels, groups_loci):\n",
    "    #print(test_index)\n",
    "    # get the training and test data\n",
    "    Xlan_train, Xlan_test = features_esm2[train_index], features_esm2[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    imbalance = sum([1 for i in y_train if i==1]) / sum([1 for i in y_train if i==0])\n",
    "\n",
    "    ## ESM-2 EMBEDDINGS: XGBoost model\n",
    "    xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=7,\n",
    "                        n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "    xgb.fit(Xlan_train, y_train)\n",
    "    score_xgb = xgb.predict_proba(Xlan_test)[:,1]\n",
    "    scores_lan.append(score_xgb)\n",
    "    \n",
    "    # save labels for later\n",
    "    label_list.append(y_test)\n",
    "    \n",
    "    # pbar update\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "logo_results = {'labels': label_list, 'scores_language': scores_lan}   \n",
    "with open(results_path+'/v3.4/combined_logocv_results_v34_'+threshold_str+'.pickle', 'wb') as f:\n",
    "    pickle.dump(logo_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748536a",
   "metadata": {},
   "source": [
    "## 5. Results interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "with open(results_path+'/v3.4/combined_logocv_results_v34_'+threshold_str+'.pickle', 'rb') as f:\n",
    "    logo_results = pickle.load(f)\n",
    "scores_lan = logo_results['scores_language']\n",
    "label_list = logo_results['labels']\n",
    "\n",
    "# compute performance\n",
    "rqueries_lan = []\n",
    "for i in range(len(set(groups_loci))):\n",
    "    score_lan = scores_lan[i]\n",
    "    y_test = label_list[i]\n",
    "    try:\n",
    "            roc_auc = roc_auc_score(y_test, score_lan)\n",
    "            ranked_lan = [x for _, x in sorted(zip(score_lan, y_test), reverse=True)]\n",
    "            rqueries_lan.append(ranked_lan)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f6035",
   "metadata": {},
   "source": [
    "#### ROC AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, ROC AUC \n",
    "labels = np.concatenate(label_list).ravel()\n",
    "scoreslr = np.concatenate(scores_lan).ravel()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "fpr, tpr, thrs = roc_curve(labels, scoreslr)\n",
    "rauclr = round(auc(fpr, tpr), 3)\n",
    "ax.plot(fpr, tpr, c='#124559', linewidth=2.5, label='ESM-2 + XGBoost (AUC= '+str(rauclr)+')')\n",
    "ax.set_xlabel('False positive rate', size=24)\n",
    "ax.set_ylabel('True positive rate', size=24)\n",
    "ax.legend(loc=4, prop={'size': 20})\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.savefig(results_path+'/vbeta/logocv_rocauc.png', dpi=400)\n",
    "fig.savefig(results_path+'/vbeta/logocv_rocauc_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13828f4a",
   "metadata": {},
   "source": [
    "#### Hit ratio against microbiologist approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dce071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "# prep the data\n",
    "interactions1 = general_path+'/klebsiella_phage_host_interactions.xlsx'\n",
    "interactions2 = general_path+'/klebsiella_interactions_part2.xlsx' # for part 1 NO SUGGESTIONS POSSIBLE -> ALL UNIQUE K-TYPES\n",
    "matrix1 = pd.read_excel(interactions1, index_col=0, header=0)\n",
    "matrix2 = pd.read_excel(interactions2, index_col=0, header=0)\n",
    "locipath = general_path+'/LocibaseValencia.json'\n",
    "seros = pd.read_csv(general_path+'/serotypesValencia.csv')\n",
    "with open(locipath) as f:\n",
    "    locibase = json.load(f)\n",
    "\n",
    "# do the informed approach\n",
    "hits = {i: 0 for i in range(1, 51)}\n",
    "total = 0\n",
    "# --------------------\n",
    "# MATRIX 1\n",
    "# --------------------\n",
    "loci_serotype = {}\n",
    "for i, accession in enumerate(locibase.keys()):\n",
    "    loci_serotype[accession] = seros['sero'][i]\n",
    "    \n",
    "# phages sorted by broad-spec\n",
    "sorted_phages = matrix1.sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# delete keys not in this matrix (only suggestions within the matrix)\n",
    "rownames = list(matrix1.index.values)\n",
    "no_genome = ['K2', 'K21', 'K23', 'K27', 'K28', 'K40', 'K45', 'K48', 'K52', 'K53', 'K67', 'K69', 'K70', 'K71', 'K72']\n",
    "rownames = [str(i) for i in rownames if i not in no_genome]\n",
    "for key in list(loci_serotype.keys()):\n",
    "    if key not in rownames:\n",
    "        del loci_serotype[key]\n",
    "        \n",
    "# iterate over all accessions in matrix1\n",
    "for i, accession in enumerate(rownames):\n",
    "    # only compute hit ratio when we can find something\n",
    "    if sum(matrix1.loc[accession]) > 0:\n",
    "        # get the serotype\n",
    "        serotype = loci_serotype[str(accession)]\n",
    "        # search other bacteria with the same serotype\n",
    "        same_serotype = [key for key, value in loci_serotype.items() if value == serotype]\n",
    "        same_serotype.remove(str(accession))\n",
    "        # get phage suggestions: columnnames of corresponding bacteria in matrix1 with value = 1\n",
    "        phage_suggestions = []\n",
    "        for j, acc in enumerate(same_serotype):\n",
    "            if acc in ['132', '779', '806', '228', '245', '406', '1210', '1446', '1468', '1572', '2164']:\n",
    "                acc = int(acc)\n",
    "            colnames = matrix1.columns[matrix1.loc[acc] == 1].tolist()\n",
    "            phage_suggestions.append(colnames)\n",
    "        # flatten the list\n",
    "        phage_suggestions = list(set([item for sublist in phage_suggestions for item in sublist]))\n",
    "        # sort the list based: most narrow phages first!\n",
    "        phage_suggestions.sort(key=lambda x: matrix1[x].sum(), reverse=True)\n",
    "        \n",
    "        total += 1\n",
    "        for k in range(1, 51):\n",
    "            # approach 1: if we dont have enough suggestions, pick extra at random from total pool available\n",
    "            # approach 2: now, we supplement them with the sorted phages by broad-spectrum, not random!\n",
    "            if k > len(phage_suggestions):\n",
    "                sample_pool = [sugg for sugg in sorted_phages if sugg not in phage_suggestions]\n",
    "                to_pick = k-len(phage_suggestions)\n",
    "                if len(sample_pool) < to_pick:\n",
    "                    phage_suggestions = phage_suggestions + sample_pool\n",
    "                else:\n",
    "                    phage_suggestions = phage_suggestions + sample_pool[:to_pick]\n",
    "\n",
    "            #suggested = random.sample(phage_suggestions, k)\n",
    "            if any([matrix1.loc[accession, sugg] == 1 for sugg in phage_suggestions]):\n",
    "                hits[k] += 1\n",
    "                \n",
    "# --------------------\n",
    "# MATRIX 2\n",
    "# --------------------\n",
    "hits2 = {i: 0 for i in range(1, 51)}\n",
    "total2 = 0\n",
    "loci_serotype = {}\n",
    "for i, accession in enumerate(locibase.keys()):\n",
    "    loci_serotype[accession] = seros['sero'][i]\n",
    "    \n",
    "sorted_phages = matrix2.sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# delete keys not in this matrix (only suggestions within the matrix)\n",
    "rownames = list(matrix2.index.values)\n",
    "rownames = [str(i) for i in rownames]\n",
    "for key in list(loci_serotype.keys()):\n",
    "    if key not in rownames:\n",
    "        del loci_serotype[key]\n",
    "\n",
    "# iterate over all accessions in matrix2\n",
    "for i, accession in enumerate(matrix2.index.values):\n",
    "    # only compute hit ratio when we can find something\n",
    "    if sum(matrix2.loc[accession]) > 0:\n",
    "        # get the serotype\n",
    "        serotype = loci_serotype[str(accession)]\n",
    "        # search other bacteria with the same serotype\n",
    "        same_serotype = [key for key, value in loci_serotype.items() if value == serotype]\n",
    "        same_serotype.remove(str(accession))\n",
    "        # get phage suggestions: columnnames of corresponding bacteria in matrix2 with value = 1\n",
    "        phage_suggestions = []\n",
    "        for j, acc in enumerate(same_serotype):\n",
    "            if acc in ['132', '779', '806', '228', '245', '406', '1210', '1446', '1468', '1572', '2164']:\n",
    "                acc = int(acc)\n",
    "            colnames = matrix2.columns[matrix2.loc[acc] == 1].tolist()\n",
    "            phage_suggestions.append(colnames)\n",
    "        # flatten the list\n",
    "        phage_suggestions = list(set([item for sublist in phage_suggestions for item in sublist]))\n",
    "        # sort the list based: most narrow phages first!\n",
    "        phage_suggestions.sort(key=lambda x: matrix2[x].sum(), reverse=True)\n",
    "\n",
    "        total += 1\n",
    "        total2 += 1\n",
    "        for k in range(1, 51):\n",
    "            # if we dont have enough suggestions, pick extra at random from the total pool\n",
    "            if k > len(phage_suggestions):\n",
    "                sample_pool = [sugg for sugg in sorted_phages if sugg not in phage_suggestions]\n",
    "                to_pick = k-len(phage_suggestions)\n",
    "                if len(sample_pool) < to_pick:\n",
    "                    phage_suggestions = phage_suggestions + sample_pool\n",
    "                else:\n",
    "                    phage_suggestions = phage_suggestions + sample_pool[:to_pick]\n",
    "            \n",
    "            if any([matrix2.loc[accession, sugg] == 1 for sugg in phage_suggestions]):\n",
    "                hits[k] += 1\n",
    "                hits2[k] += 1\n",
    "\n",
    "informed_hitratio = {k: v/total for k, v in hits.items()}\n",
    "informed_hitratio2 = {k: v/total2 for k, v in hits2.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd02253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, hit ratios @ K\n",
    "ks = np.linspace(1, 50, 50)\n",
    "hits_lan = [phlu.hitratio(rqueries_lan, int(k)) for k in ks]\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(ks, hits_lan, c='#124559', linewidth=2.5, label='ESM-2 + XGBoost')\n",
    "#ax.plot(ks, hits_ens, c='#124559', linewidth=2.5, label='Combined model')\n",
    "#ax.plot(ks, hits_random, c='#81B29A', linewidth=2.5, ls=':', label='Random guess')\n",
    "#ax.plot(ks, list(informed_hitratio.values()), c='#E15554', linewidth=2.5, ls='-.', label='Informed microbiologist')\n",
    "#ax.plot(ks, list(informed_hitratio2.values()), c='#E15554', linewidth=2.5, ls=':', label='Informed guess (Bea only)')\n",
    "ax.set_xlabel('$\\it{k}$', size=24)\n",
    "ax.set_ylabel('Hit ratio @ $\\it{k}$', size=24)\n",
    "ax.set_ylim(0.1, 1)\n",
    "ax.legend(loc=4, prop={'size': 24})\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.savefig(results_path+'/vbeta/logocv_hitratio_informed.png', dpi=400)\n",
    "fig.savefig(results_path+'/vbeta/logocv_hitratio_informed_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e1da9",
   "metadata": {},
   "source": [
    "#### Performance per K-type\n",
    "\n",
    "https://medium.com/@curryrowan/simplified-logistic-regression-classification-with-categorical-variables-in-python-1ce50c4b137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "with open(results_path+'/v3.4/combined_logocv_results_v34_100.pickle', 'rb') as f:\n",
    "    logo_results = pickle.load(f)\n",
    "scores_lan = logo_results['scores_language']\n",
    "label_list = logo_results['labels']\n",
    "\n",
    "# read K-types\n",
    "seros = pd.read_csv(general_path+'/serotypesValencia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean hit ratio per K-type\n",
    "unique_seros = list(set(seros['sero']))\n",
    "performance_ktypes = {}\n",
    "labelcount_ktypes = {}\n",
    "for unique in unique_seros:\n",
    "    indices = seros['sero'] == unique\n",
    "    subscores_lan = [val for is_good, val in zip(indices, scores_lan) if is_good]\n",
    "    sublabels = [val for is_good, val in zip(indices, label_list) if is_good]\n",
    "    labelcount_ktypes[unique] = [sum(i) for i in sublabels]\n",
    "    rqueries_lan = []\n",
    "    for i in range(len(subscores_lan)):\n",
    "        score_lan = subscores_lan[i]\n",
    "        y_test = sublabels[i]\n",
    "        if sum(y_test) > 0:\n",
    "            ranked_lan = [x for _, x in sorted(zip(score_lan, y_test), reverse=True)]\n",
    "            rqueries_lan.append(ranked_lan)\n",
    "    if len(rqueries_lan) > 0:\n",
    "        hr_lan = round(phlu.hitratio(rqueries_lan, 10), 3)\n",
    "        performance_ktypes[unique] = [('HR_XGB', hr_lan)]\n",
    "    #else:\n",
    "    #    performance_ktypes[unique] = [('MAR_XGB', np.nan), ('MAR_HDC', np.nan), ('MAR_COMBINED', np.nan)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_hr_xgb = []\n",
    "for ktype in performance_ktypes:\n",
    "    performance_hr_xgb.append(performance_ktypes[ktype][0][1])\n",
    "sortedpairs = [(x,y) for y, x in sorted(zip(performance_hr_xgb, list(performance_ktypes.keys())), reverse=True)]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.hist(performance_hr_xgb, bins=25, color='#124559')\n",
    "#sns.barplot(x=[score for (key, score) in sortedpairs], y=[key for (key, score) in sortedpairs], ax=ax, palette='magma')\n",
    "ax.set_xlabel('Mean top-10 hit ratio', size=22)\n",
    "ax.set_ylabel('Number of K-types', size=22)\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.tight_layout()\n",
    "fig.savefig(results_path+'/vbeta/histogram_ktypes_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf0916",
   "metadata": {},
   "source": [
    "#### Hit ratio per K-type versus number of pos labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e921059",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = [x[0] for x in sortedpairs if x[1] == 1] # all with HR == 1\n",
    "bottom10 = [x[0] for x in sortedpairs if x[1] == 0] # all with HR == 0\n",
    "middle = [x[0] for x in sortedpairs if (x[1] != 0 and x[1] != 1)]\n",
    "countst10 = []\n",
    "countsb10 = []\n",
    "countsmid = []\n",
    "for key in labelcount_ktypes.keys():\n",
    "    if key in top10:\n",
    "        countst10.append(labelcount_ktypes[key])\n",
    "    elif key in bottom10:\n",
    "        countsb10.append(labelcount_ktypes[key])\n",
    "    elif key in middle:\n",
    "        countsmid.append(labelcount_ktypes[key])\n",
    "countst10 = [i for x in countst10 for i in x]\n",
    "countsb10 = [i for x in countsb10 for i in x]\n",
    "countsmid = [i for x in countsmid for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "countlist = [countst10, countsmid, countsb10]\n",
    "binlist = [15, 15, 15]\n",
    "\n",
    "for i, count in enumerate(countlist):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.histplot(count, ax=ax, color='#221150', bins=binlist[i])\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel('Number of confirmed interactions', size=22)\n",
    "    ax.set_ylabel('Number of bacteria', size=22)\n",
    "    ax.yaxis.set_tick_params(labelsize = 14)\n",
    "    ax.xaxis.set_tick_params(labelsize = 14)\n",
    "    fig.savefig(results_path+'/v3.4/ktypecounts_svg'+str(i)+'.svg', format='svg', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
