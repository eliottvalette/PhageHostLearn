{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e97849c",
   "metadata": {},
   "source": [
    "## 1. Initial set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import math\n",
    "from os import listdir\n",
    "\n",
    "# Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SearchIO import HmmerIO\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Features\n",
    "import torch\n",
    "import esm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60092310",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "\n",
    "The data processing of PhageHostLearn consists of five consecutive steps: (1) phage gene calling with PHANOTATE, (2) phage protein embedding with bio_embeddings, (3) phage RBP detection, (4) bacterial genome processing with Kaptive and (5) processing the interaction matrix.\n",
    "\n",
    "Expected outputs: (1) an RBPbase.csv file with detected RBPs, (2) a Locibase.json file with detected K-loci proteins, (3) a .csv file of processed interaction data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b686db",
   "metadata": {},
   "source": [
    "#### 2.1 PHANOTATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phanotate_processing(phage_genomes_path, phanotate_path, output_path, add=False, test=False, num_phages=None):\n",
    "    \"\"\"Run PHANOTATE on each phage genome and build the gene database.\"\"\"\n",
    "    \n",
    "    # Get the number of files in the phage genomes directory\n",
    "    print(f\"  Processing {len(listdir(phage_genomes_path))} phage files...\")\n",
    "    phage_files = listdir(phage_genomes_path)\n",
    "    print(f'  Number of phage files: {len(phage_files)}')\n",
    "    \n",
    "    # Remove MacOS system file if present\n",
    "    if '.DS_Store' in phage_files:\n",
    "        phage_files.remove('.DS_Store')\n",
    "    \n",
    "    # If add=True, filter out already processed phages\n",
    "    if add:\n",
    "        rbp_base = pd.read_csv('data/RBPbase.csv')\n",
    "        phage_ids = list(set(rbp_base['phage_ID']))\n",
    "        \n",
    "        # Remove phages already in RBPbase from the list of phages to process\n",
    "        phage_files = [x for x in phage_files if x.split('.fasta')[0] not in phage_ids]\n",
    "        print(f'  Processing {len(phage_files)} more phages (add=True)')\n",
    "    \n",
    "    # Optionally limit to a subset of phages (for testing only)\n",
    "    if num_phages is not None:\n",
    "        print(f'  Processing only the first {num_phages} phages')\n",
    "        phage_files = phage_files[:num_phages]\n",
    "    \n",
    "    # Create progress bar\n",
    "    bar = tqdm(total=len(phage_files), position=0, leave=True, desc='  Processing phage genomes')\n",
    "    name_list = []\n",
    "    gene_list = []\n",
    "    gene_ids = []\n",
    "\n",
    "    # Process each phage file\n",
    "    for file in phage_files:\n",
    "        count = 1\n",
    "        file_dir = phage_genomes_path + '/' + file\n",
    "        \n",
    "        # Construct PHANOTATE command\n",
    "        raw_str = phanotate_path + ' ' + file_dir\n",
    "        \n",
    "        # Run PHANOTATE as a subprocess and capture output\n",
    "        process = subprocess.Popen(raw_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "        stdout, _ = process.communicate()\n",
    "        stdout_text = stdout.decode('utf-8', errors='ignore')\n",
    "        \n",
    "        # Check for PHANOTATE execution errors\n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(\n",
    "                f\"PHANOTATE command failed for {file_dir} with exit code {process.returncode}. \"\n",
    "                f\"Command output:\\n{stdout_text}\"\n",
    "            )\n",
    "        \n",
    "        # Skip the PHANOTATE header (first 2 lines)\n",
    "        std_splits = stdout.split(sep=b'\\n')[2:]\n",
    "        \n",
    "        # If no ORFs found, raise error (Open Reading Frame). It's the portions of the sequences.\n",
    "        if not any(split.strip() for split in std_splits):\n",
    "            raise ValueError(\n",
    "                f\"PHANOTATE did not return ORF predictions for {file_dir}. \"\n",
    "                f\"Command output:\\n{stdout_text}\"\n",
    "            )\n",
    "        \n",
    "        # Write table to a temporary TSV file, clean up commas if present\n",
    "        temp_tab_path = os.path.join('../data/phage_results.tsv')\n",
    "        with open(temp_tab_path, 'wb') as temp_tab:\n",
    "            for split in std_splits:\n",
    "                split = split.replace(b',', b'')\n",
    "                temp_tab.write(split + b'\\n')\n",
    "        \n",
    "        # Load PHANOTATE result as a pandas DataFrame\n",
    "        try:\n",
    "            results_orfs = pd.read_csv(temp_tab_path, sep='\\t', lineterminator='\\n', index_col=False)\n",
    "        except pd.errors.EmptyDataError as exc:\n",
    "            with open(temp_tab_path, 'r', encoding='utf-8', errors='ignore') as temp_in:\n",
    "                temp_preview = temp_in.read()\n",
    "            raise ValueError(\n",
    "                f\"PHANOTATE output for {file_dir} produced an empty or invalid TSV file.\\n\"\n",
    "                f\"Command output:\\n{stdout_text}\\n\"\n",
    "                f\"Temporary TSV content:\\n{temp_preview}\"\n",
    "            ) from exc\n",
    "        \n",
    "        # Parse phage file name (without .fasta extension)\n",
    "        name = file.split('.fasta')[0]\n",
    "        \n",
    "        # Fetch full nucleotide sequence of the phage\n",
    "        sequence = str(SeqIO.read(file_dir, 'fasta').seq)\n",
    "        \n",
    "        # Iterate over ORFs\n",
    "        for j, strand in enumerate(results_orfs['FRAME']):\n",
    "            start = results_orfs['#START'][j]\n",
    "            stop = results_orfs['STOP'][j]\n",
    "            if strand == '+':\n",
    "        \n",
    "                # Forward strand: take subsequence from start to stop (1-based, inclusive)\n",
    "                gene = sequence[start - 1:stop]\n",
    "            else:\n",
    "        \n",
    "                # Reverse strand: take subsequence, then reverse complement\n",
    "                sequence_part = sequence[stop - 1:start]\n",
    "                gene = str(Seq(sequence_part).reverse_complement())\n",
    "            name_list.append(name)\n",
    "            gene_list.append(gene)\n",
    "            gene_ids.append(name + '_gp' + str(count))\n",
    "            count += 1\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    # Clean up temporary file if not in test mode\n",
    "    if not test and os.path.exists('../data/phage_results.tsv'):\n",
    "        os.remove('../data/phage_results.tsv')\n",
    "\n",
    "    # Build DataFrame of all predicted gene sequences\n",
    "    genebase_df = pd.DataFrame(list(zip(name_list, gene_ids, gene_list)), columns=['phage_ID', 'gene_ID', 'gene_sequence'])\n",
    "    \n",
    "    # If add=True, concatenate with previous gene database\n",
    "    if add:\n",
    "        old_genebase_df = pd.read_csv(output_path)\n",
    "        genebase_df = pd.concat([old_genebase_df, genebase_df], axis=0)\n",
    "    \n",
    "    # Save to CSV\n",
    "    genebase_df.to_csv(output_path, index=False)\n",
    "    print(f'  Completed PHANOTATE - Number of phage genes: {len(genebase_df)}')\n",
    "    return genebase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d633972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 2.1] Running PHANOTATE on phage genomes...\")\n",
    "phage_genomes_path = '../data/phages_genomes'\n",
    "phanotate_path = '/Users/eliottvalette/Documents/Clones/PhageHostLearn/.venv/bin/phanotate.py'\n",
    "output_path = '../data/phage_genes.csv'\n",
    "genebase_df = phanotate_processing(phage_genomes_path, phanotate_path, output_path, num_phages=2)\n",
    "genebase_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a48369",
   "metadata": {},
   "source": [
    "#### 2.2 Protein embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd322693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_protein_embeddings(genebase_path, add=False, num_genes=None):\n",
    "    \"\"\"\n",
    "    Compute protein embeddings for phage gene sequences using ProtTransBertBFD.\n",
    "\n",
    "    Parameters:\n",
    "    - data_suffix (str): Optional string to add as suffix to the output embeddings filename.\n",
    "    - add (bool): If True, only compute embeddings for new genes and add to existing file.\n",
    "    - num_genes (int or None): If set, only process the first num_genes genes in phage_genes.csv.\n",
    "\n",
    "    Returns:\n",
    "    - None. Embedding data is saved as a CSV file at the computed path.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load phage gene base table\n",
    "    genebase = pd.read_csv(genebase_path)\n",
    "    # If num_genes is set, use only the first num_genes genes for embedding\n",
    "    if num_genes is not None:\n",
    "        print(f'  Processing only the first {num_genes} phage genes')\n",
    "        genebase = genebase.head(num_genes)\n",
    "    print(f'  Number of phage genes: {len(genebase)}')\n",
    "    \n",
    "    # Import the embedding model from bio_embeddings\n",
    "    print('  Importing ProtTransBertBFDEmbedder...')\n",
    "    from bio_embeddings.embed import ProtTransBertBFDEmbedder\n",
    "    print('  Import successful')\n",
    "    \n",
    "    # Initialize the embedder timing the operation\n",
    "    time_start = time.time()\n",
    "    embedder = ProtTransBertBFDEmbedder()\n",
    "    time_end = time.time()\n",
    "    print(f'  Time taken to initialize embedder: {time_end - time_start:.2f} seconds')\n",
    "    print('  Embedder initialized')\n",
    "    \n",
    "    # If adding new sequences to existing embeddings\n",
    "    if add:\n",
    "        print('  Adding new protein embeddings')\n",
    "        # Load the existing embeddings DataFrame (must exist when add=True)\n",
    "        old_embeddings_df = pd.read_csv('../data/phage_protein_embeddings.csv')\n",
    "        protein_ids = list(old_embeddings_df['ID'])\n",
    "        sequences = []\n",
    "        names = []\n",
    "        # For each gene, if it is not already embedded, add it to the embedding queue\n",
    "        for i, sequence in enumerate(genebase['gene_sequence']):\n",
    "            if genebase['gene_ID'][i] not in protein_ids:\n",
    "                # Translate DNA to protein sequence and strip last stop codon (*)\n",
    "                sequences.append(str(Seq(sequence).translate())[:-1])\n",
    "                names.append(genebase['gene_ID'][i])\n",
    "    else:\n",
    "        print('  Computing protein embeddings for all phage genes')\n",
    "        # Take all gene IDs as names\n",
    "        names = list(genebase['gene_ID'])\n",
    "        print(f'  Number of protein sequences to embed: {len(names)}')\n",
    "        # Translate all DNA sequences to protein sequences, minus last stop codon\n",
    "        sequences = [str(Seq(sequence).translate())[:-1] for sequence in genebase['gene_sequence']]\n",
    "\n",
    "    print(f'  Number of protein sequences to embed: {len(sequences)}')\n",
    "    protein_embeddings = []\n",
    "    \n",
    "    # Progress bar for embedding computation\n",
    "    progress_bar = tqdm(sequences, desc='  Computing protein embeddings', unit='protein')\n",
    "    for protein_sequence in progress_bar:\n",
    "        \n",
    "        # Compute the embedding for each protein sequence and reduce to a fixed-size vector\n",
    "        reduced_embedding = embedder.reduce_per_protein(embedder.embed(protein_sequence))\n",
    "        protein_embeddings.append(reduced_embedding)\n",
    "    \n",
    "    # Combine names and embeddings into a DataFrame\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'ID': names}), pd.DataFrame(protein_embeddings)], axis=1)\n",
    "    \n",
    "    # If add = True, vertically stack the old and new DataFrames (NP vstack ensures correct shape)\n",
    "    if add:\n",
    "        embeddings_df = pd.DataFrame(np.vstack([old_embeddings_df, embeddings_df]), columns=old_embeddings_df.columns)\n",
    "    \n",
    "    # Save embedding DataFrame to CSV file\n",
    "    embeddings_df.to_csv('../data/phage_protein_embeddings.csv', index=False)\n",
    "    print(f'  Protein embeddings saved to: ../data/phage_protein_embeddings.csv')\n",
    "    return embeddings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62489ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbpbase_path = '../data/RBPbase.csv'\n",
    "\n",
    "if os.path.exists(rbpbase_path) :\n",
    "    print(\"\\n[STEP 2.2] Computing protein embeddings with ProtTransBertBFD...\")\n",
    "    print(\"  RBPbase already exists - skipping protein embeddings computation.\")\n",
    "    embeddings_df = pd.read_csv('../data/phage_protein_embeddings.csv')\n",
    "else :\n",
    "    embeddings_df = compute_protein_embeddings(rbpbase_path, num_genes=5)\n",
    "\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07fe07",
   "metadata": {},
   "source": [
    "#### 2.3 PhageRBPdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a181bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmmpress_python(hmm_path, pfam_file):\n",
    "    \"\"\"Press a profiles database, necessary to do scanning.\"\"\"\n",
    "    # Create command to change directory and run hmmpress on the Pfam file\n",
    "    cd_str = 'cd ' + hmm_path  # Change to the HMMER directory\n",
    "    press_str = 'hmmpress ' + pfam_file  # Command to press the HMM database\n",
    "    command = cd_str + '; ' + press_str  # Combine commands with ';'\n",
    "    # Run the hmmpress command in a subprocess\n",
    "    press_process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    press_out, press_err = press_process.communicate()  # Capture output and error\n",
    "    return press_out, press_err  # Return the output and error\n",
    "\n",
    "\n",
    "def single_hmmscan_python(hmm_path, pfam_file, fasta_file):\n",
    "    \"\"\"Run hmmscan for a given FASTA file of one (or multiple) sequences.\"\"\"\n",
    "    # Change current directory to the HMMER directory\n",
    "    cd_str = 'cd ' + hmm_path\n",
    "    cd_process = subprocess.Popen(cd_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    cd_process.communicate()\n",
    "    # Run hmmscan on the fasta_file using the pfam_file database, save results to text file\n",
    "    scan_str = 'hmmscan ' + pfam_file + ' ' + fasta_file + ' > hmmscan_out.txt'\n",
    "    scan_process = subprocess.Popen(scan_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    scan_process.communicate()\n",
    "    # Parse results\n",
    "    with open('hmmscan_out.txt') as results_handle:\n",
    "        scan_res = HmmerIO.Hmmer3TextParser(results_handle)\n",
    "    # Remove output file\n",
    "    os.remove('hmmscan_out.txt')\n",
    "    return scan_res  # Return parsed scan results\n",
    "\n",
    "\n",
    "def hmmscan_python(hmm_path, pfam_file, sequences_file, threshold=18):\n",
    "    \"\"\"Scan sequences for domains using hmmscan.\"\"\"\n",
    "    domains = []  # List to store domain names\n",
    "    scores = []   # List to store bitscores\n",
    "    biases = []   # List to store bias scores\n",
    "    ranges = []   # List to store alignment ranges\n",
    "    # Iterate over each sequence in the fasta file\n",
    "    for sequence in SeqIO.parse(sequences_file, 'fasta'):\n",
    "        # Write single sequence to a temporary fasta file\n",
    "        with open('single_sequence.fasta', 'w') as temp_fasta:\n",
    "            temp_fasta.write('>' + sequence.id + '\\n' + str(sequence.seq) + '\\n')\n",
    "\n",
    "        # Run hmmscan on the single sequence\n",
    "        scan_res = single_hmmscan_python(hmm_path, pfam_file, 'single_sequence.fasta')\n",
    "        # Iterate over results lines and hits\n",
    "        for line in scan_res:\n",
    "            try:\n",
    "                for hit in line.hits:\n",
    "                    hsp = hit._items[0]  # Take the first HSP (high scoring pair)\n",
    "                    aln_start = hsp.query_range[0]\n",
    "                    aln_stop = hsp.query_range[1]\n",
    "                    # Only add unique domains with score >= threshold\n",
    "                    if (hit.bitscore >= threshold) and (hit.id not in domains):\n",
    "                        domains.append(hit.id)\n",
    "                        scores.append(hit.bitscore)\n",
    "                        biases.append(hit.bias)\n",
    "                        ranges.append((aln_start, aln_stop))\n",
    "            except IndexError:\n",
    "                # If there are no hits, skip\n",
    "                pass\n",
    "    # Remove the temporary fasta file after processing\n",
    "    os.remove('single_sequence.fasta')\n",
    "    return domains, scores, biases, ranges  # Return results\n",
    "\n",
    "\n",
    "def gene_domain_scan(hmmpath, pfam_file, gene_hits, threshold=18):\n",
    "    \"\"\"Run hmmscan on translated gene hits.\"\"\"\n",
    "    # Write each gene hit (translated) to a temporary protein fasta file\n",
    "    with open('protein_hits.fasta', 'w') as hits_fasta:\n",
    "        for i, gene_hit in enumerate(gene_hits):\n",
    "            protein_sequence = str(Seq(gene_hit).translate())[:-1]\n",
    "            hits_fasta.write('>' + str(i) + '_proteindomain_hit\\n' + protein_sequence + '\\n')\n",
    "    # Scan the protein fasta file with hmmscan\n",
    "    domains, scores, biases, ranges = hmmscan_python(hmmpath, pfam_file, 'protein_hits.fasta', threshold)\n",
    "    # Remove the temporary fasta file\n",
    "    os.remove('protein_hits.fasta')\n",
    "    return domains, scores, biases, ranges  # Return scan results\n",
    "\n",
    "\n",
    "def phageRBPdetect(pfam_path, hmmer_path, xgb_path, gene_embeddings_path):\n",
    "    \"\"\"Detect receptor-binding proteins using PhageRBPdetect.\"\"\"\n",
    "    print(f\"  Loading genebase from {'data/phage_genes.csv'}\")\n",
    "    genebase = pd.read_csv('data/phage_genes.csv')  # Load phage genes from CSV\n",
    "    # List of all possible functional blocks/domains of interest\n",
    "    new_blocks = ['Phage_T7_tail', 'Tail_spike_N', 'Prophage_tail', 'BppU_N', 'Mtd_N', 'Head_binding', 'DUF3751',\n",
    "                  'End_N_terminal', 'phage_tail_N', 'Prophage_tailD1', 'DUF2163', 'Phage_fiber_2', 'unknown_N0',\n",
    "                  'unknown_N1', 'unknown_N2', 'unknown_N3', 'unknown_N4', 'unknown_N6', 'unknown_N10', 'unknown_N11',\n",
    "                  'unknown_N12', 'unknown_N13', 'unknown_N17', 'unknown_N19', 'unknown_N23', 'unknown_N24',\n",
    "                  'unknown_N26', 'unknown_N29', 'unknown_N36', 'unknown_N45', 'unknown_N48', 'unknown_N49',\n",
    "                  'unknown_N53', 'unknown_N57', 'unknown_N60', 'unknown_N61', 'unknown_N65', 'unknown_N73',\n",
    "                  'unknown_N82', 'unknown_N83', 'unknown_N101', 'unknown_N114', 'unknown_N119', 'unknown_N122',\n",
    "                  'unknown_N163', 'unknown_N174', 'unknown_N192', 'unknown_N200', 'unknown_N206', 'unknown_N208',\n",
    "                  'Lipase_GDSL_2', 'Pectate_lyase_3', 'gp37_C', 'Beta_helix', 'Gp58', 'End_beta_propel',\n",
    "                  'End_tail_spike', 'End_beta_barrel', 'PhageP22-tail', 'Phage_spike_2', 'gp12-short_mid', 'Collar',\n",
    "                  'unknown_C2', 'unknown_C3', 'unknown_C8', 'unknown_C15', 'unknown_C35', 'unknown_C54', 'unknown_C76',\n",
    "                  'unknown_C100', 'unknown_C105', 'unknown_C112', 'unknown_C123', 'unknown_C179', 'unknown_C201',\n",
    "                  'unknown_C203', 'unknown_C228', 'unknown_C234', 'unknown_C242', 'unknown_C258', 'unknown_C262',\n",
    "                  'unknown_C267', 'unknown_C268', 'unknown_C274', 'unknown_C286', 'unknown_C292', 'unknown_C294',\n",
    "                  'Peptidase_S74', 'Phage_fiber_C', 'S_tail_recep_bd', 'CBM_4_9', 'DUF1983', 'DUF3672']\n",
    "\n",
    "    print(\"  Pressing HMM database...\")\n",
    "    # Make sure the profile HMM database is indexed for hmmscan\n",
    "    output, err = hmmpress_python(hmmer_path, pfam_path)\n",
    "    print(output.decode('utf-8', errors='ignore'))\n",
    "\n",
    "    phage_genes = genebase['gene_sequence']  # Get gene sequences\n",
    "    # Initialize domain score matrix with zeros\n",
    "    hmm_scores = {item: [0] * len(phage_genes) for item in new_blocks}\n",
    "    # Progress bar for scanning all genes\n",
    "    bar = tqdm(total=len(phage_genes), position=0, leave=True, desc='  Scanning phage genes')\n",
    "    for i, sequence in enumerate(phage_genes):\n",
    "        # For each gene, scan for domains\n",
    "        hits, scores, biases, ranges = gene_domain_scan(hmmer_path, pfam_path, [sequence])\n",
    "        # Save scores for detected domains\n",
    "        for j, dom in enumerate(hits):\n",
    "            hmm_scores[dom][i] = scores[j]\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    print(\"  Loading embeddings and computing features...\")\n",
    "    embeddings_df = pd.read_csv(gene_embeddings_path)  # Load protein embeddings for each gene\n",
    "    embeddings = np.asarray(embeddings_df.iloc[:, 1:])  # Get the embedding vectors (without gene_ID)\n",
    "    hmm_scores_array = np.asarray(pd.DataFrame(hmm_scores))  # Convert domain scores to array\n",
    "    # Concatenate features: embedding + domain scores\n",
    "    features = np.concatenate((embeddings, hmm_scores_array), axis=1)\n",
    "\n",
    "    print(\"  Loading XGBoost model for RBP detection...\")\n",
    "    xgb_saved = XGBClassifier()  # Initialize XGBoost classifier\n",
    "    xgb_saved.load_model(xgb_path)  # Load pretrained model from disk\n",
    "\n",
    "    print(\"  Making predictions...\")\n",
    "    score_xgb = xgb_saved.predict_proba(features)[:, 1]  # Get probability for class 1 (RBP)\n",
    "    preds_xgb = (score_xgb > 0.5) * 1  # Apply threshold for classification\n",
    "\n",
    "    # Prepare results for output\n",
    "    rbp_base = {'phage_ID': [], 'protein_ID': [], 'protein_sequence': [], 'dna_sequence': [], 'xgb_score': []}\n",
    "    # Filter genes predicted as RBP (pred = 1)\n",
    "    for i, dna_sequence in enumerate(genebase['gene_sequence']):\n",
    "        if preds_xgb[i] == 1:\n",
    "            rbp_base['phage_ID'].append(genebase['phage_ID'][i])\n",
    "            rbp_base['protein_ID'].append(genebase['gene_ID'][i])\n",
    "            rbp_base['protein_sequence'].append(str(Seq(dna_sequence).translate())[:-1])  # Translate DNA to protein (remove stop)\n",
    "            rbp_base['dna_sequence'].append(dna_sequence)\n",
    "            rbp_base['xgb_score'].append(score_xgb[i])\n",
    "    rbp_base_df = pd.DataFrame(rbp_base)\n",
    "    # Remove short/long proteins (<200 or >1500 aa)\n",
    "    to_delete = [i for i, protein_seq in enumerate(rbp_base_df['protein_sequence']) if (len(protein_seq) < 200 or len(protein_seq) > 1500)]\n",
    "    rbp_base_df = rbp_base_df.drop(to_delete).reset_index(drop=True)\n",
    "    # Save results to CSV\n",
    "    rbp_base_df.to_csv('data/RBPbase.csv', index=False)\n",
    "    print(f\"  RBP detection completed - Found {len(rbp_base_df)} RBPs\")\n",
    "    return rbp_base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 PhageRBPdetect\n",
    "print(\"\\n[STEP 2.3] Running PhageRBPdetect...\")\n",
    "\n",
    "# Check if RBPbase already exists\n",
    "rbpbase_path = os.path.join(rbpbase_path)\n",
    "\n",
    "if os.path.exists(rbpbase_path) :\n",
    "    print(\"  RBPbase already exists - skipping PhageRBPdetect.\")\n",
    "    rbp_base_df = pd.read_csv('data/RBPbase.csv')\n",
    "else:\n",
    "    root_path = 'path/to/root'\n",
    "    pfam_path = os.path.join(root_path, 'RBPdetect_phageRBPs.hmm')\n",
    "    if not os.path.exists(pfam_path):\n",
    "        pfam_path = os.path.join(root_path, 'code', 'RBPdetect_phageRBPs.hmm')\n",
    "    \n",
    "    # Use hmmer-3.4 from data directory\n",
    "    hmmer_path = os.path.join(root_path, 'hmmer-3.4')\n",
    "    if not os.path.exists(hmmer_path):\n",
    "        raise FileNotFoundError(f\"HMMER directory not found at {hmmer_path}\")\n",
    "    \n",
    "    xgb_path = os.path.join(root_path, 'RBPdetect_xgb_hmm.json')\n",
    "    if not os.path.exists(xgb_path):\n",
    "        xgb_path = os.path.join(root_path, 'code', 'RBPdetect_xgb_hmm.json')\n",
    "    \n",
    "    gene_embeddings_path = os.path.join(root_path, f'phage_protein_embeddings.csv')\n",
    "    \n",
    "    if not os.path.exists(gene_embeddings_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Protein embeddings file not found at {gene_embeddings_path}. \"\n",
    "            f\"Please compute protein embeddings first or ensure the file exists.\"\n",
    "        )\n",
    "    \n",
    "    rbp_base_df = phageRBPdetect(pfam_path, hmmer_path, xgb_path, gene_embeddings_path)\n",
    "\n",
    "rbp_base_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed4b0c",
   "metadata": {},
   "source": [
    "#### 2.4 Kaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ab96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaptive_python(database_path, file_path, output_dir='../data'):\n",
    "    # Wrapper for Kaptive CLI\n",
    "    command = (f\"python kaptive.py -a {file_path} -k {database_path} -o {output_dir}/ --no_table\")\n",
    "    ssprocess = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    stdout, _ = ssprocess.communicate()\n",
    "    if ssprocess.returncode != 0:\n",
    "        raise RuntimeError(f\"Kaptive failed for {file_path}.\\nCmd: {command}\\nOutput:\\n{stdout.decode(errors='ignore')}\")\n",
    "    return\n",
    "\n",
    "def process_bacterial_genomes(bact_genomes_path, database_path, output_dir = '../data', add = False, num_phages = None):\n",
    "    \"\"\"\n",
    "    Run Kaptive on a set of bacterial genome FASTA files and build:\n",
    "      - serotypes.csv  (one serotype per accession)\n",
    "      - Locibase.json  (list of locus protein sequences per accession)\n",
    "      - kaptive_results_all_loci.fasta (concatenated locus sequences)\n",
    "    \"\"\"\n",
    "\n",
    "    # List FASTA files deterministically to get reproducible subsets\n",
    "    fastas = sorted(listdir(bact_genomes_path))\n",
    "    # Keep only the files that ends with .fasta\n",
    "    fastas = [f for f in fastas if f.endswith('.fasta')]\n",
    "\n",
    "    # If we're appending, filter out accessions already processed\n",
    "    if add:\n",
    "        locibase_path = os.path.join(output_dir, 'Locibase.json')\n",
    "        if not os.path.exists(locibase_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"add=True but {locibase_path} does not exist. Run once with add=False first.\"\n",
    "            )\n",
    "        with open(locibase_path) as dict_file:\n",
    "            old_locibase = json.load(dict_file)\n",
    "        already_done = set(old_locibase.keys())\n",
    "        fastas = [x for x in fastas if x.split('.fasta')[0] not in already_done]\n",
    "        print(f'  Processing {len(fastas)} more bacteria (add=True)')\n",
    "\n",
    "    # Optionally limit the run to a subset (after filtering)\n",
    "    if num_phages is not None:\n",
    "        fastas = fastas[:num_phages]\n",
    "        print(f'  Limiting to the first {num_phages} genomes')\n",
    "\n",
    "    accessions = [file.split('.fasta')[0] for file in fastas]\n",
    "    serotypes = []\n",
    "    loci_results = {}\n",
    "\n",
    "    # Progress bar over the selected genomes\n",
    "    pbar = tqdm(total=len(fastas), desc='  Processing bacterial genomes')\n",
    "\n",
    "    # Concatenated locus sequences will be saved here\n",
    "    big_fasta_path = os.path.join(output_dir, 'kaptive_results_all_loci.fasta')\n",
    "    with open(big_fasta_path, 'w') as big_fasta:\n",
    "        for i, file in enumerate(fastas):\n",
    "            file_path = os.path.join(bact_genomes_path, file)\n",
    "\n",
    "            # Run Kaptive for this genome\n",
    "            kaptive_python(database_path, file_path, output_dir=output_dir)\n",
    "\n",
    "            # Kaptive should produce these two artifacts in output_dir for each input:\n",
    "            #  - kaptive_results.json (structured summary)\n",
    "            #  - kaptive_results_<input_filename> (FASTA of locus sequences)\n",
    "            json_path = os.path.join(output_dir, 'kaptive_results.json')\n",
    "            locus_fasta = os.path.join(output_dir, f'kaptive_results_{file}')\n",
    "\n",
    "            if not os.path.exists(json_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Expected Kaptive output not found: {json_path}\"\n",
    "                )\n",
    "            if not os.path.exists(locus_fasta):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Expected Kaptive locus FASTA not found: {locus_fasta}\"\n",
    "                )\n",
    "\n",
    "            # Parse JSON: serotype and per-gene protein sequences\n",
    "            with open(json_path, 'r') as jf:\n",
    "                results = json.load(jf)\n",
    "\n",
    "            # Serotype of the best locus match\n",
    "            serotypes.append(results[0]['Best match']['Type'])\n",
    "\n",
    "            # Extract protein sequences; prefer tblastn result over reference when available\n",
    "            for gene in results[0]['Locus genes']:\n",
    "                try:\n",
    "                    protein = gene['tblastn result']['Protein sequence']\n",
    "                    protein = protein.replace('-', '').replace('*', '')\n",
    "                except KeyError:\n",
    "                    protein = gene['Reference']['Protein sequence']\n",
    "                loci_results.setdefault(accessions[i], []).append(protein[:-1])  # trim trailing stop if any\n",
    "\n",
    "            # Concatenate all locus records for this genome into one long sequence\n",
    "            loci_sequence = ''\n",
    "            for record in SeqIO.parse(locus_fasta, 'fasta'):\n",
    "                loci_sequence += str(record.seq)\n",
    "            big_fasta.write(f'>{accessions[i]}\\n{loci_sequence}\\n')\n",
    "\n",
    "            # Clean up intermediates produced by BLAST and Kaptive\n",
    "            for extension in ['.ndb', '.not', '.ntf', '.nto']:\n",
    "                tmp_path = file_path + extension\n",
    "                if os.path.exists(tmp_path):\n",
    "                    os.remove(tmp_path)\n",
    "            if os.path.exists(json_path):\n",
    "                os.remove(json_path)\n",
    "            if os.path.exists(locus_fasta):\n",
    "                os.remove(locus_fasta)\n",
    "\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # Build/update serotype table\n",
    "    sero_df = pd.DataFrame(serotypes, columns=['sero'])\n",
    "    if add:\n",
    "        # Merge with existing results\n",
    "        loci_results = {**old_locibase, **loci_results}\n",
    "        sero_csv = os.path.join(output_dir, 'serotypes.csv')\n",
    "        if os.path.exists(sero_csv):\n",
    "            old_seros = pd.read_csv(sero_csv)\n",
    "            sero_df = pd.concat([old_seros, sero_df], axis=0)\n",
    "\n",
    "    # Persist final artifacts\n",
    "    sero_df.to_csv(os.path.join(output_dir, 'serotypes.csv'), index=False)\n",
    "    with open(os.path.join(output_dir, 'Locibase.json'), 'w') as dict_file:\n",
    "        json.dump(loci_results, dict_file)\n",
    "\n",
    "    print(f\"  Kaptive processing completed - Processed {len(accessions)} bacteria\")\n",
    "    return sero_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4255c",
   "metadata": {},
   "source": [
    "#### 2.5 Process the interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be319ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[STEP 2.4] Running Kaptive on bacterial genomes...\")\n",
    "\n",
    "sero_df = process_bacterial_genomes(\n",
    "    bact_genomes_path='../data/klebsiella_genomes/fasta_files',\n",
    "    database_path='../data/Klebsiella_k_locus_primary_reference.gbk',\n",
    "    output_dir='../data',\n",
    "    add=False,\n",
    "    num_phages=2\n",
    ")\n",
    "\n",
    "sero_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13771b",
   "metadata": {},
   "source": [
    "## 3. Feature construction\n",
    "\n",
    "Starts from the RBPbase.csv and the Locibase.json files that should be stored in the general_path. If you wish to reproduce our analyses, you can download these files from our [Zenodo repository](https://doi.org/10.5281/zenodo.8095914).\n",
    "\n",
    "Expected outputs: (1) a .csv file with RBP embeddings, (2) a .csv file with loci embeddings. The last function outputs the following Python objects: ESM-2 feature matrix, labels, groups_loci and groups_phage (for evaluation). If the ESM-2 embeddings take too long, you might opt to do this step in the cloud or on a high-performance computer.\n",
    "\n",
    "If you're retraining a model with the same data but new validated interactions, you can simply run the `construct_feature_matrices` function to construct updated feature matrices and labels and train models anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adda9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_esm2_embeddings_rbp(add=False):\n",
    "    \"\"\"\n",
    "    This function computes ESM-2 embeddings for the RBPs, from the RBPbase.csv file.\n",
    "\n",
    "    INPUTS:\n",
    "    - general path to the project data folder\n",
    "    - data suffix to optionally add to the saved file name (default='')\n",
    "    OUTPUT: esm2_embeddings_rbp.csv\n",
    "    \"\"\"\n",
    "    # load the ESM2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    # get the correct data to embed\n",
    "    RBPbase = pd.read_csv('../data/RBPbase.csv')\n",
    "    if add == True:\n",
    "        old_embeddings_df = pd.read_csv('../data/esm2_embeddings_rbp.csv')\n",
    "        protein_ids = list(set(old_embeddings_df['protein_ID']))\n",
    "        to_delete = [i for i, prot_id in enumerate(RBPbase['protein_ID']) if prot_id in protein_ids]\n",
    "        RBPbase = RBPbase.drop(to_delete)\n",
    "        RBPbase = RBPbase.reset_index(drop=True)\n",
    "        print('Processing ', len(RBPbase['protein_sequence']), ' more sequences (add=True)')\n",
    "\n",
    "    # loop over data and embed (batch size = 1)\n",
    "    bar = tqdm(total=len(RBPbase['protein_sequence']), position=0, leave=True)\n",
    "    sequence_representations = []\n",
    "    for i, sequence in enumerate(RBPbase['protein_sequence']):\n",
    "        data = [(RBPbase['protein_ID'][i], sequence)]\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        with torch.no_grad():\n",
    "            results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "        for j, (_, seq) in enumerate(data):\n",
    "            sequence_representations.append(token_representations[j, 1 : len(seq) + 1].mean(0))\n",
    "        bar.update(1)\n",
    "    bar.close()\n",
    "\n",
    "    # save results\n",
    "    phage_ids = RBPbase['phage_ID']\n",
    "    ids = RBPbase['protein_ID']\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'phage_ID':phage_ids}), pd.DataFrame({'protein_ID':ids}), pd.DataFrame(sequence_representations).astype('float')], axis=1)\n",
    "    if add == True:\n",
    "        embeddings_df = pd.DataFrame(np.vstack([old_embeddings_df, embeddings_df]), columns=old_embeddings_df.columns)\n",
    "    embeddings_df.to_csv('../data/esm2_embeddings_rbp.csv', index=False)\n",
    "    return embeddings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34687caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_embeddings_rbp_df_path = '../data/esm2_embeddings_rbp.csv'\n",
    "\n",
    "if os.path.exists(esm2_embeddings_rbp_df_path) :\n",
    "    print(\"\\n[STEP 3.1] Computing ESM 2 embeddings RBP...\")\n",
    "    print(\"  esm2_embeddings_rbp_df already exists - skipping ...\")\n",
    "    esm2_embeddings_rbp_df = pd.read_csv('../data/esm2_embeddings_rbp.csv')\n",
    "else :\n",
    "    esm2_embeddings_rbp_df = compute_esm2_embeddings_rbp()\n",
    "\n",
    "esm2_embeddings_rbp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee2bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_esm2_embeddings_loci(add=False):\n",
    "    \"\"\"\n",
    "    This function computes ESM-2 embeddings for the loci proteins, from the Locibase.json file.\n",
    "\n",
    "    INPUTS:\n",
    "    - general path to the project data folder\n",
    "    - data suffix to optionally add to the saved file name (default='')\n",
    "    OUTPUT: esm2_embeddings_loci.csv\n",
    "    \"\"\"\n",
    "    # Load ESM-2 model\n",
    "    model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    # Load json file\n",
    "    dict_file = open('../data/Locibase.json')\n",
    "    loci_dict = json.load(dict_file)\n",
    "    if add == True:\n",
    "        old_embeddings_df = pd.read_csv('../data/esm2_embeddings_loci.csv')\n",
    "        old_accessions = list(set(old_embeddings_df['accession']))\n",
    "        for key in loci_dict.keys():\n",
    "            if key in old_accessions:\n",
    "                del loci_dict[key]\n",
    "        print('Processing ', len(loci_dict.keys()), ' more bacteria (add=True)')\n",
    "\n",
    "    # loop over data and embed (batch size = 1)\n",
    "    loci_representations = []\n",
    "    for key in tqdm(loci_dict.keys()):\n",
    "        embeddings = []\n",
    "        for sequence in loci_dict[key]:\n",
    "            data = [(key, sequence)]\n",
    "            batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "            for i, (_, seq) in enumerate(data):\n",
    "                embeddings.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "        locus_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "        loci_representations.append(locus_embedding)\n",
    "\n",
    "    # save results\n",
    "    embeddings_df = pd.concat([pd.DataFrame({'accession':list(loci_dict.keys())}), pd.DataFrame(loci_representations)], axis=1)\n",
    "    if add == True:\n",
    "        embeddings_df = pd.DataFrame(np.vstack([old_embeddings_df, embeddings_df]), columns=old_embeddings_df.columns)\n",
    "    embeddings_df.to_csv('../data/esm2_embeddings_loci.csv', index=False)\n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94526a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm2_embeddings_loci_df_path = '../data/esm2_embeddings_loci.csv'\n",
    "\n",
    "if os.path.exists(esm2_embeddings_loci_df_path) :\n",
    "    print(\"\\n[STEP 3.2] Computing ESM 2 embeddings RBP...\")\n",
    "    print(\"  esm2_embeddings_loci_df already exists - skipping ...\")\n",
    "    esm2_embeddings_loci_df = pd.read_csv('../data/esm2_embeddings_loci.csv')\n",
    "else :\n",
    "    esm2_embeddings_loci_df = compute_esm2_embeddings_loci()\n",
    "\n",
    "esm2_embeddings_loci_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af80705",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbp_embeddings_path = '../data/esm2_embeddings_rbp.csv'\n",
    "loci_embeddings_path = '../data/esm2_embeddings_loci.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_matrices(lociembeddings_path, rbpembeddings_path, mode='train'):\n",
    "    \"\"\"\n",
    "    This function constructs two corresponding feature matrices ready for machine learning, \n",
    "    starting from the ESM-2 embeddings of RBPs and loci proteins.\n",
    "\n",
    "    INPUTS:\n",
    "    - path: general or test path depending on the mode\n",
    "    - suffix: general or test suffix depending on the mode\n",
    "    - lociembeddings path to the loci embeddings csv file\n",
    "    - rbpembeddings path to the rbp embeddings csv file\n",
    "    - mode: 'train' or 'test', test mode doesn't use an IM (default='train')\n",
    "    OUTPUT: features_esm2, labels, groups_loci, groups_phage\n",
    "    \"\"\"\n",
    "    RBP_embeddings = pd.read_csv(rbpembeddings_path)\n",
    "    loci_embeddings = pd.read_csv(lociembeddings_path)\n",
    "    if mode == 'train':\n",
    "        interactions = pd.read_csv('../data/phage_host_interactions.csv', index_col=0)\n",
    "\n",
    "    # construct multi-RBP representations\n",
    "    multi_embeddings = []\n",
    "    names = []\n",
    "    for phage_id in list(set(RBP_embeddings['phage_ID'])):\n",
    "        rbp_embeddings = RBP_embeddings.iloc[:,2:][RBP_embeddings['phage_ID'] == phage_id]\n",
    "        multi_embedding = np.mean(np.asarray(rbp_embeddings), axis=0)\n",
    "        names.append(phage_id)\n",
    "        multi_embeddings.append(multi_embedding)\n",
    "    multiRBP_embeddings = pd.concat([pd.DataFrame({'phage_ID': names}), pd.DataFrame(multi_embeddings)], axis=1)\n",
    "\n",
    "    # construct dataframe for training\n",
    "    features_lan = []\n",
    "    labels = []\n",
    "    groups_loci = []\n",
    "    groups_phage = []\n",
    "\n",
    "    for i, accession in enumerate(loci_embeddings['accession']):\n",
    "        for j, phage_id in enumerate(multiRBP_embeddings['phage_ID']):\n",
    "            if mode == 'train':\n",
    "                interaction = interactions.loc[accession][phage_id]\n",
    "                if math.isnan(interaction) == False: # if the interaction is known\n",
    "                    # language embeddings\n",
    "                    features_lan.append(pd.concat([loci_embeddings.iloc[i, 1:], multiRBP_embeddings.iloc[j, 1:]]))\n",
    "\n",
    "                    # append labels and groups\n",
    "                    labels.append(int(interaction))\n",
    "                    groups_loci.append(i)\n",
    "                    groups_phage.append(j)\n",
    "            elif mode == 'test':\n",
    "                # language embeddings\n",
    "                features_lan.append(pd.concat([loci_embeddings.iloc[i, 1:], multiRBP_embeddings.iloc[j, 1:]]))\n",
    "                \n",
    "                # append groups\n",
    "                groups_loci.append(i)\n",
    "                groups_phage.append(j)\n",
    "\n",
    "                \n",
    "    features_lan = np.asarray(features_lan)\n",
    "    print(\"Dimensions match?\", features_lan.shape[1] == (loci_embeddings.shape[1]+multiRBP_embeddings.shape[1]-2))\n",
    "\n",
    "    #np.save(general_path+'/esm2_features'+data_suffix+'.txt', features_lan)\n",
    "    if mode == 'train':\n",
    "        return features_lan, labels, groups_loci, groups_phage\n",
    "    elif mode == 'test':\n",
    "        return features_lan, groups_loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_esm2, labels, groups_loci, groups_phage = construct_feature_matrices(loci_embeddings_path, rbp_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83b57a",
   "metadata": {},
   "source": [
    "## 4. Training and evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import phagehostlearn_utils as phlu\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve, roc_curve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416719a1",
   "metadata": {},
   "source": [
    "#### 4.1 Training both models and saving them for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus=6\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM-2 FEATURES + XGBoost model\n",
    "imbalance = sum([1 for i in labels if i==1]) / sum([1 for i in labels if i==0])\n",
    "xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=7,\n",
    "                    n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb.fit(features_esm2, labels)\n",
    "xgb.save_model('phagehostlearn_vbeta.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0143c",
   "metadata": {},
   "source": [
    "#### 4.2 LOGOCV with the combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da198e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to set a threshold for grouping\n",
    "matrix = np.loadtxt('../data/all_loci_score_matrix.txt', delimiter='\\t')\n",
    "threshold = 0.995\n",
    "threshold_str='995'\n",
    "group_i = 0\n",
    "new_groups = [np.nan] * len(groups_loci)\n",
    "for i in range(matrix.shape[0]):\n",
    "    cluster = np.where(matrix[i,:] >= threshold)[0]\n",
    "    oldgroups_i = [k for k, x in enumerate(groups_loci) if x in cluster]\n",
    "    if np.isnan(new_groups[groups_loci.index(i)]):\n",
    "        for ogi in oldgroups_i:\n",
    "            new_groups[ogi] = group_i\n",
    "        group_i += 1\n",
    "groups_loci = new_groups\n",
    "print('Number of unique groups: ', len(set(groups_loci)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4aad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "cpus = 6\n",
    "scores_lan = []\n",
    "label_list = []\n",
    "labels = np.asarray(labels)\n",
    "pbar = tqdm(total=len(set(groups_loci)))\n",
    "for train_index, test_index in logo.split(features_esm2, labels, groups_loci):\n",
    "    #print(test_index)\n",
    "    # get the training and test data\n",
    "    Xlan_train, Xlan_test = features_esm2[train_index], features_esm2[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    imbalance = sum([1 for i in y_train if i==1]) / sum([1 for i in y_train if i==0])\n",
    "\n",
    "    ## ESM-2 EMBEDDINGS: XGBoost model\n",
    "    xgb = XGBClassifier(scale_pos_weight=1/imbalance, learning_rate=0.3, n_estimators=250, max_depth=7,\n",
    "                        n_jobs=cpus, eval_metric='logloss', use_label_encoder=False)\n",
    "    xgb.fit(Xlan_train, y_train)\n",
    "    score_xgb = xgb.predict_proba(Xlan_test)[:,1]\n",
    "    scores_lan.append(score_xgb)\n",
    "    \n",
    "    # save labels for later\n",
    "    label_list.append(y_test)\n",
    "    \n",
    "    # pbar update\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "logo_results = {'labels': label_list, 'scores_language': scores_lan}   \n",
    "with open('../results/v3.4/combined_logocv_results_v34_'+threshold_str+'.pickle', 'wb') as f:\n",
    "    pickle.dump(logo_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748536a",
   "metadata": {},
   "source": [
    "## 5. Results interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71a31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "with open('../results/v3.4/combined_logocv_results_v34_'+threshold_str+'.pickle', 'rb') as f:\n",
    "    logo_results = pickle.load(f)\n",
    "scores_lan = logo_results['scores_language']\n",
    "label_list = logo_results['labels']\n",
    "\n",
    "# compute performance\n",
    "rqueries_lan = []\n",
    "for i in range(len(set(groups_loci))):\n",
    "    score_lan = scores_lan[i]\n",
    "    y_test = label_list[i]\n",
    "    try:\n",
    "            roc_auc = roc_auc_score(y_test, score_lan)\n",
    "            ranked_lan = [x for _, x in sorted(zip(score_lan, y_test), reverse=True)]\n",
    "            rqueries_lan.append(ranked_lan)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f6035",
   "metadata": {},
   "source": [
    "#### ROC AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, ROC AUC \n",
    "labels = np.concatenate(label_list).ravel()\n",
    "scoreslr = np.concatenate(scores_lan).ravel()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "fpr, tpr, thrs = roc_curve(labels, scoreslr)\n",
    "rauclr = round(auc(fpr, tpr), 3)\n",
    "ax.plot(fpr, tpr, c='#124559', linewidth=2.5, label='ESM-2 + XGBoost (AUC= '+str(rauclr)+')')\n",
    "ax.set_xlabel('False positive rate', size=24)\n",
    "ax.set_ylabel('True positive rate', size=24)\n",
    "ax.legend(loc=4, prop={'size': 20})\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.savefig('../results/vbeta/logocv_rocauc.png', dpi=400)\n",
    "fig.savefig('../results/vbeta/logocv_rocauc_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13828f4a",
   "metadata": {},
   "source": [
    "#### Hit ratio against microbiologist approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dce071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Hit ratio against microbiologist approach (CSV only) ----\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "data_dir = '../data'\n",
    "\n",
    "# Real files (CSV)\n",
    "interactions1_csv = os.path.join(data_dir, 'phage_host_interactions.csv')\n",
    "interactions2_csv = os.path.join(data_dir, 'klebsiella_interactions_part2.csv')  # optional\n",
    "locipath = os.path.join(data_dir, 'Locibase.json')\n",
    "serotypes_csv = os.path.join(data_dir, 'serotypes.csv')\n",
    "\n",
    "# Loading files\n",
    "matrix1 = pd.read_csv(interactions1_csv, index_col=0)\n",
    "matrix2 = pd.read_csv(interactions2_csv, index_col=0) if os.path.exists(interactions2_csv) else None\n",
    "\n",
    "with open(locipath) as f:\n",
    "    locibase = json.load(f)\n",
    "seros = pd.read_csv(serotypes_csv)\n",
    "\n",
    "# --------------------\n",
    "# Common preparation\n",
    "# --------------------\n",
    "hits = {i: 0 for i in range(1, 51)}\n",
    "hits2 = {i: 0 for i in range(1, 51)}\n",
    "total = 0\n",
    "total2 = 0\n",
    "\n",
    "# Associate a serotype to each accession (order of seros aligned with Locibase)\n",
    "loci_serotype = {}\n",
    "for i, accession in enumerate(locibase.keys()):\n",
    "    loci_serotype[accession] = seros['sero'][i]\n",
    "\n",
    "# --------------------\n",
    "# MATRIX 1 (csv)\n",
    "# --------------------\n",
    "sorted_phages = matrix1.sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "# Same exclusions as in your original code\n",
    "no_genome = ['K2', 'K21', 'K23', 'K27', 'K28', 'K40', 'K45', 'K48', 'K52', 'K53', 'K67', 'K69', 'K70', 'K71', 'K72']\n",
    "rownames = [str(i) for i in list(matrix1.index.values) if i not in no_genome]\n",
    "\n",
    "# Keep only the keys present in the matrix\n",
    "for key in list(loci_serotype.keys()):\n",
    "    if key not in rownames:\n",
    "        del loci_serotype[key]\n",
    "\n",
    "for accession in rownames:\n",
    "    if sum(matrix1.loc[accession]) > 0:\n",
    "        serotype = loci_serotype[str(accession)]\n",
    "        same_serotype = [key for key, value in loci_serotype.items() if value == serotype and key != str(accession)]\n",
    "\n",
    "        # suggestions = columns with 1 for other accessions of same serotype\n",
    "        phage_suggestions = []\n",
    "        for acc in same_serotype:\n",
    "            acc_cast = int(acc) if acc in ['132','779','806','228','245','406','1210','1446','1468','1572','2164'] else acc\n",
    "            colnames = matrix1.columns[matrix1.loc[acc_cast] == 1].tolist() if acc_cast in matrix1.index else []\n",
    "            phage_suggestions.append(colnames)\n",
    "\n",
    "        phage_suggestions = list(set([p for sub in phage_suggestions for p in sub]))\n",
    "        phage_suggestions.sort(key=lambda x: matrix1[x].sum(), reverse=True)\n",
    "\n",
    "        total += 1\n",
    "        for k in range(1, 51):\n",
    "            if k > len(phage_suggestions):\n",
    "                # complete with the broadest phages if not enough suggestions\n",
    "                pool = [s for s in sorted_phages if s not in phage_suggestions]\n",
    "                phage_suggestions = phage_suggestions + pool[:(k - len(phage_suggestions))]\n",
    "            if any(matrix1.loc[accession, sugg] == 1 for sugg in phage_suggestions):\n",
    "                hits[k] += 1\n",
    "\n",
    "# --------------------\n",
    "# MATRIX 2 (csv if available)\n",
    "# --------------------\n",
    "if matrix2 is not None:\n",
    "    loci_serotype2 = {}\n",
    "    for i, accession in enumerate(locibase.keys()):\n",
    "        loci_serotype2[accession] = seros['sero'][i]\n",
    "\n",
    "    sorted_phages2 = matrix2.sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    rownames2 = [str(i) for i in list(matrix2.index.values)]\n",
    "    for key in list(loci_serotype2.keys()):\n",
    "        if key not in rownames2:\n",
    "            del loci_serotype2[key]\n",
    "\n",
    "    for accession in matrix2.index.values:\n",
    "        if sum(matrix2.loc[accession]) > 0:\n",
    "            serotype = loci_serotype2[str(accession)]\n",
    "            same_serotype = [key for key, value in loci_serotype2.items() if value == serotype and key != str(accession)]\n",
    "\n",
    "            phage_suggestions = []\n",
    "            for acc in same_serotype:\n",
    "                acc_cast = int(acc) if acc in ['132','779','806','228','245','406','1210','1446','1468','1572','2164'] else acc\n",
    "                colnames = matrix2.columns[matrix2.loc[acc_cast] == 1].tolist() if acc_cast in matrix2.index else []\n",
    "                phage_suggestions.append(colnames)\n",
    "\n",
    "            phage_suggestions = list(set([p for sub in phage_suggestions for p in sub]))\n",
    "            phage_suggestions.sort(key=lambda x: matrix2[x].sum(), reverse=True)\n",
    "\n",
    "            total += 1\n",
    "            total2 += 1\n",
    "            for k in range(1, 51):\n",
    "                if k > len(phage_suggestions):\n",
    "                    pool = [s for s in sorted_phages2 if s not in phage_suggestions]\n",
    "                    phage_suggestions = phage_suggestions + pool[:(k - len(phage_suggestions))]\n",
    "                if any(matrix2.loc[accession, sugg] == 1 for sugg in phage_suggestions):\n",
    "                    hits[k] += 1\n",
    "                    hits2[k] += 1\n",
    "\n",
    "# Ratios\n",
    "informed_hitratio  = {k: (hits[k]  / total)  if total  > 0 else np.nan for k in hits}\n",
    "informed_hitratio2 = {k: (hits2[k] / total2) if total2 > 0 else np.nan for k in hits2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd02253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results, hit ratios @ K\n",
    "ks = np.linspace(1, 50, 50)\n",
    "hits_lan = [phlu.hitratio(rqueries_lan, int(k)) for k in ks]\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(ks, hits_lan, c='#124559', linewidth=2.5, label='ESM-2 + XGBoost')\n",
    "ax.set_xlabel('$\\it{k}$', size=24)\n",
    "ax.set_ylabel('Hit ratio @ $\\it{k}$', size=24)\n",
    "ax.set_ylim(0.1, 1)\n",
    "ax.legend(loc=4, prop={'size': 24})\n",
    "ax.grid(True, linestyle=':')\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.savefig('../results/vbeta/logocv_hitratio_informed.png', dpi=400)\n",
    "fig.savefig('../results/vbeta/logocv_hitratio_informed_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e1da9",
   "metadata": {},
   "source": [
    "#### Performance per K-type\n",
    "\n",
    "https://medium.com/@curryrowan/simplified-logistic-regression-classification-with-categorical-variables-in-python-1ce50c4b137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results\n",
    "with open('../results/v3.4/combined_logocv_results_v34_100.pickle', 'rb') as f:\n",
    "    logo_results = pickle.load(f)\n",
    "scores_lan = logo_results['scores_language']\n",
    "label_list = logo_results['labels']\n",
    "\n",
    "# read K-types\n",
    "seros = pd.read_csv(general_path+'/serotypesValencia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean hit ratio per K-type\n",
    "unique_seros = list(set(seros['sero']))\n",
    "performance_ktypes = {}\n",
    "labelcount_ktypes = {}\n",
    "for unique in unique_seros:\n",
    "    indices = seros['sero'] == unique\n",
    "    subscores_lan = [val for is_good, val in zip(indices, scores_lan) if is_good]\n",
    "    sublabels = [val for is_good, val in zip(indices, label_list) if is_good]\n",
    "    labelcount_ktypes[unique] = [sum(i) for i in sublabels]\n",
    "    rqueries_lan = []\n",
    "    for i in range(len(subscores_lan)):\n",
    "        score_lan = subscores_lan[i]\n",
    "        y_test = sublabels[i]\n",
    "        if sum(y_test) > 0:\n",
    "            ranked_lan = [x for _, x in sorted(zip(score_lan, y_test), reverse=True)]\n",
    "            rqueries_lan.append(ranked_lan)\n",
    "    if len(rqueries_lan) > 0:\n",
    "        hr_lan = round(phlu.hitratio(rqueries_lan, 10), 3)\n",
    "        performance_ktypes[unique] = [('HR_XGB', hr_lan)]\n",
    "    #else:\n",
    "    #    performance_ktypes[unique] = [('MAR_XGB', np.nan), ('MAR_HDC', np.nan), ('MAR_COMBINED', np.nan)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_hr_xgb = []\n",
    "for ktype in performance_ktypes:\n",
    "    performance_hr_xgb.append(performance_ktypes[ktype][0][1])\n",
    "sortedpairs = [(x,y) for y, x in sorted(zip(performance_hr_xgb, list(performance_ktypes.keys())), reverse=True)]\n",
    "fig, ax = plt.subplots(figsize=(16,6))\n",
    "ax.hist(performance_hr_xgb, bins=25, color='#124559')\n",
    "#sns.barplot(x=[score for (key, score) in sortedpairs], y=[key for (key, score) in sortedpairs], ax=ax, palette='magma')\n",
    "ax.set_xlabel('Mean top-10 hit ratio', size=22)\n",
    "ax.set_ylabel('Number of K-types', size=22)\n",
    "ax.yaxis.set_tick_params(labelsize = 14)\n",
    "ax.xaxis.set_tick_params(labelsize = 14)\n",
    "fig.tight_layout()\n",
    "fig.savefig('../results/vbeta/histogram_ktypes_svg.svg', format='svg', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf0916",
   "metadata": {},
   "source": [
    "#### Hit ratio per K-type versus number of pos labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e921059",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = [x[0] for x in sortedpairs if x[1] == 1] # all with HR == 1\n",
    "bottom10 = [x[0] for x in sortedpairs if x[1] == 0] # all with HR == 0\n",
    "middle = [x[0] for x in sortedpairs if (x[1] != 0 and x[1] != 1)]\n",
    "countst10 = []\n",
    "countsb10 = []\n",
    "countsmid = []\n",
    "for key in labelcount_ktypes.keys():\n",
    "    if key in top10:\n",
    "        countst10.append(labelcount_ktypes[key])\n",
    "    elif key in bottom10:\n",
    "        countsb10.append(labelcount_ktypes[key])\n",
    "    elif key in middle:\n",
    "        countsmid.append(labelcount_ktypes[key])\n",
    "countst10 = [i for x in countst10 for i in x]\n",
    "countsb10 = [i for x in countsb10 for i in x]\n",
    "countsmid = [i for x in countsmid for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "countlist = [countst10, countsmid, countsb10]\n",
    "binlist = [15, 15, 15]\n",
    "\n",
    "for i, count in enumerate(countlist):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    sns.histplot(count, ax=ax, color='#221150', bins=binlist[i])\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel('Number of confirmed interactions', size=22)\n",
    "    ax.set_ylabel('Number of bacteria', size=22)\n",
    "    ax.yaxis.set_tick_params(labelsize = 14)\n",
    "    ax.xaxis.set_tick_params(labelsize = 14)\n",
    "    fig.savefig('../results/v3.4/ktypecounts_svg'+str(i)+'.svg', format='svg', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
